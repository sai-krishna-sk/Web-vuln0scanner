import requests
from urllib.parse import urljoin, urlparse, urlunparse
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import re
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import logging
import threading

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WebCrawler:
    def __init__(self, max_links=50, max_threads=20, timeout=5, max_depth=3):
        self.max_links = max_links
        self.max_threads = max_threads  # Increased from 8
        self.timeout = timeout  # Reduced from 15
        self.max_depth = max_depth
        self.discovered_urls = set()
        self.visited_urls = set()
        self.lock = threading.Lock()
        
        # Create session with speed optimizations
        self.session = requests.Session()
        
        # Minimal retry strategy for speed
        retry_strategy = Retry(
            total=1,  # Reduced from 2
            status_forcelist=[429, 500, 502, 503, 504],
            method_whitelist=["HEAD", "GET", "OPTIONS"],
            backoff_factor=0.2,  # Reduced from 0.5
            read=1,  # Reduced from 2
            connect=1  # Reduced from 2
        )
        
        # Optimized adapter for speed
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=25,  # Increased
            pool_maxsize=25,  # Increased
            pool_block=False
        )
        
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Minimal headers for speed
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,*/*;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        })
        
    def normalize_url(self, url):
        """Fast URL normalization"""
        try:
            parsed = urlparse(url)
            path = parsed.path.rstrip('/') if parsed.path != '/' else '/'
            return urlunparse((
                parsed.scheme,
                parsed.netloc.lower(),
                path,
                parsed.params,
                parsed.query,
                ''
            ))
        except:
            return url
    
    def is_valid_url(self, url, base_domain):
        """Optimized URL validation"""
        try:
            parsed = urlparse(url)
            base_parsed = urlparse(base_domain)
            
            # Quick domain check
            if parsed.netloc.lower() != base_parsed.netloc.lower():
                return False
            
            # Quick extension check - reduced list for speed
            path_lower = parsed.path.lower()
            if any(path_lower.endswith(ext) for ext in ['.pdf', '.doc', '.jpg', '.png', '.gif', '.mp4', '.zip', '.css', '.js']):
                return False
            
            # Quick pattern check - reduced list for speed
            if any(pattern in path_lower for pattern in ['/wp-admin/', '/admin/login', '.xml', '.json']):
                return False
                
            return True
        except:
            return False
    
    def extract_links_from_html(self, soup, base_url):
        """Optimized HTML link extraction"""
        links = set()
        
        # Limit processing for speed
        for tag in soup.find_all(['a', 'area'], href=True, limit=30):
            href = tag.get('href', '').strip()
            if href and not href.startswith(('#', 'javascript:', 'mailto:', 'tel:')):
                full_url = urljoin(base_url, href)
                normalized_url = self.normalize_url(full_url)
                if self.is_valid_url(normalized_url, base_url):
                    links.add(normalized_url)
        
        # Quick form extraction - limited for speed
        for form in soup.find_all('form', action=True, limit=5):
            action = form.get('action', '').strip()
            if action and not action.startswith('#'):
                full_url = urljoin(base_url, action)
                normalized_url = self.normalize_url(full_url)
                if self.is_valid_url(normalized_url, base_url):
                    links.add(normalized_url)
        
        return links
    
    def extract_links_from_js(self, html_content, base_url):
        """Fast JavaScript link extraction using regex"""
        links = set()
        
        # Simplified patterns for speed
        js_patterns = [
            r'href\s*:\s*["\']([^"\']+)["\']',
            r'location\.href\s*=\s*["\']([^"\']+)["\']'
        ]
        
        for pattern in js_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches[:10]:  # Limit for speed
                if match.startswith(('/', 'http://', 'https://')):
                    try:
                        full_url = urljoin(base_url, match)
                        normalized_url = self.normalize_url(full_url)
                        if self.is_valid_url(normalized_url, base_url):
                            links.add(normalized_url)
                    except:
                        continue
        
        return links
    
    def generate_common_paths(self, base_url):
        """Generate essential common paths only"""
        common_paths = [
            '/about', '/contact', '/services', '/products',
            '/blog', '/news', '/help', '/careers'  # Reduced list
        ]
        
        links = set()
        parsed_base = urlparse(base_url)
        
        for path in common_paths:
            full_url = f"{parsed_base.scheme}://{parsed_base.netloc}{path}"
            links.add(self.normalize_url(full_url))
            
        return links
    
    def fetch_page(self, url):
        """Fast page fetching with reduced timeout"""
        try:
            response = self.session.get(
                url, 
                timeout=(2, self.timeout),  # Reduced connect timeout
                allow_redirects=True,
                stream=False
            )
            
            if response.status_code == 200:
                final_url = self.normalize_url(response.url)
                content_type = response.headers.get('content-type', '').lower()
                
                if any(ct in content_type for ct in ['text/html', 'application/xhtml+xml']):
                    links = set()
                    
                    # Fast HTML parsing
                    soup = BeautifulSoup(response.text, 'html.parser')
                    html_links = self.extract_links_from_html(soup, final_url)
                    links.update(html_links)
                    
                    # Quick JS extraction
                    js_links = self.extract_links_from_js(response.text, final_url)
                    links.update(js_links)
                    
                    return final_url, links
                else:
                    return final_url, set()
            else:
                return None, set()
                
        except requests.exceptions.Timeout:
            return None, set()
        except requests.exceptions.RequestException:
            return None, set()
        except Exception:
            return None, set()
    
    def crawl_batch(self, urls_to_crawl, discovered_urls):
        """High-speed batch crawling"""
        new_links = set()
        successful_crawls = 0
        
        # Quick limit check
        if len(discovered_urls) >= self.max_links:
            return set()
        
        # Process all URLs in parallel with higher concurrency
        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:
            future_to_url = {executor.submit(self.fetch_page, url): url for url in urls_to_crawl}
            
            # Process results as they complete - much faster than wait()
            for future in as_completed(future_to_url, timeout=self.timeout + 10):
                with self.lock:
                    if len(discovered_urls) >= self.max_links:
                        # Cancel remaining futures
                        for f in future_to_url:
                            if not f.done():
                                f.cancel()
                        break
                
                original_url = future_to_url[future]
                try:
                    final_url, links = future.result()
                    if final_url:
                        discovered_urls.add(final_url)
                        
                        remaining_slots = self.max_links - len(discovered_urls)
                        if remaining_slots > 0:
                            limited_links = set(list(links)[:remaining_slots])
                            new_links.update(limited_links)
                        
                        successful_crawls += 1
                        logger.info(f"âœ“ ({len(discovered_urls)}/{self.max_links}): {final_url} (+{len(links)} links)")
                        
                        if len(discovered_urls) >= self.max_links:
                            break
                except:
                    continue
        
        logger.info(f"Batch: {successful_crawls}/{len(urls_to_crawl)} successful, {len(new_links)} new links")
        return new_links
    
    def crawl_domain(self, base_url):
        """Main crawling function - optimized for speed"""
        start_time = time.time()
        base_url = self.normalize_url(base_url)
        
        discovered_urls = set()
        visited_urls = set()
        
        # Start with base URL and common paths
        initial_urls = {base_url}
        common_urls = self.generate_common_paths(base_url)
        current_level_urls = initial_urls | common_urls
        
        logger.info(f"ðŸš€ SPEED CRAWL: {base_url} (Limit: {self.max_links}, Threads: {self.max_threads})")
        
        for depth in range(self.max_depth + 1):
            if len(discovered_urls) >= self.max_links:
                break
                
            if not current_level_urls:
                break
                
            logger.info(f"ðŸ“Š Depth {depth}: {len(current_level_urls)} URLs to process ({len(discovered_urls)}/{self.max_links} found)")
            
            # Filter and limit URLs
            urls_to_crawl = list(current_level_urls - visited_urls)
            remaining_slots = self.max_links - len(discovered_urls)
            urls_to_crawl = urls_to_crawl[:remaining_slots]
            
            if not urls_to_crawl:
                break
            
            visited_urls.update(urls_to_crawl)
            
            # High-speed batch processing
            new_links = self.crawl_batch(urls_to_crawl, discovered_urls)
            
            if len(discovered_urls) >= self.max_links:
                break
            
            # Prepare next level
            current_level_urls = new_links - visited_urls
        
        elapsed_time = time.time() - start_time
        result_urls = list(discovered_urls)[:self.max_links]
        
        logger.info(f"âœ… COMPLETED in {elapsed_time:.2f}s")
        logger.info(f"ðŸŽ¯ Found {len(result_urls)} URLs ({len(result_urls)/elapsed_time:.1f} URLs/sec)")
        
        # Cleanup
        try:
            self.session.close()
        except:
            pass
        
        return result_urls


def crawl_domain(base_url, max_links=50, max_threads=15, timeout=8, max_depth=2):
    """
    OPTIMIZED domain crawler - much faster than original
    
    Args:
        base_url: Starting URL to crawl
        max_links: Maximum URLs to discover (default: 50)
        max_threads: Concurrent threads (default: 15, increased from 5)
        timeout: Request timeout (default: 8s, reduced from 12s)
        max_depth: Crawling depth (default: 2, reduced from 1)
    
    Returns:
        List of discovered URLs (3-5x faster than original)
    """
    crawler = WebCrawler(
        max_links=max_links,
        max_threads=max_threads,
        timeout=timeout,
        max_depth=max_depth
    )
    
    return crawler.crawl_domain(base_url)
#curl -X POST http://localhost:5000/scan -H "Content-Type: application/json" -d "{\"url\": \"https://amrita.edu\"}"